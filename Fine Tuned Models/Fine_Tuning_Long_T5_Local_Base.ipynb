{"cells":[{"cell_type":"code","execution_count":null,"id":"0ab57300-daed-4b91-8b20-49e69c116fca","metadata":{"id":"0ab57300-daed-4b91-8b20-49e69c116fca"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","import io\n","\n","\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","import wandb"]},{"cell_type":"code","execution_count":null,"id":"5cd24365-dbd2-4103-a727-907a8b72b7c9","metadata":{"id":"5cd24365-dbd2-4103-a727-907a8b72b7c9"},"outputs":[],"source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.source_len = source_len\n","        self.summ_len = summ_len\n","        self.defense_pov = self.data.defense_pov\n","        self.judgement = self.data.judgement\n","\n","    def __len__(self):\n","        return len(self.defense_pov)\n","\n","    def __getitem__(self, index):\n","        judgement = str(self.judgement[index])\n","        judgement = ' '.join(judgement.split())\n","\n","        defense_pov = str(self.defense_pov[index])\n","        defense_pov = ' '.join(defense_pov.split())\n","\n","        source = self.tokenizer.batch_encode_plus([judgement], max_length= self.source_len,truncation=True, pad_to_max_length=True,return_tensors='pt')\n","        target = self.tokenizer.batch_encode_plus([defense_pov], max_length= self.summ_len,truncation=True, pad_to_max_length=True,return_tensors='pt')\n","\n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask'].squeeze()\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long),\n","            'source_mask': source_mask.to(dtype=torch.long),\n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_ids_y': target_ids.to(dtype=torch.long)\n","        }"]},{"cell_type":"code","execution_count":null,"id":"04f17ff6-3a08-4923-9488-ff1f6c5b5bdb","metadata":{"id":"04f17ff6-3a08-4923-9488-ff1f6c5b5bdb"},"outputs":[],"source":["def train(epoch, tokenizer, model, device, loader, optimizer):\n","    model.train()\n","    for _,data in enumerate(loader, 0):\n","        y = data['target_ids'].to(device, dtype = torch.long)\n","        y_ids = y[:, :-1].contiguous()\n","        labels = y[:, 1:].clone().detach()\n","        labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device, dtype = torch.long)\n","        mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=labels)\n","        loss = outputs[0]\n","\n","        if _%10 == 0:\n","            wandb.log({\"Training Loss\": loss.item()})\n","\n","        if _%100==0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"code","execution_count":null,"id":"c8400898-7d7e-4f47-a133-e4b13c204dd5","metadata":{"id":"c8400898-7d7e-4f47-a133-e4b13c204dd5"},"outputs":[],"source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype = torch.long)\n","            ids = data['source_ids'].to(device, dtype = torch.long)\n","            mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask,\n","                max_length=150,\n","                num_beams=2,\n","                repetition_penalty=2.5,\n","                length_penalty=1.0,\n","                early_stopping=True\n","                )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            if _%100==0:\n","                print(f'Completed {_}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","\n","    return predictions, actuals"]},{"cell_type":"code","execution_count":null,"id":"b8cc4a6d-dcf5-4a4a-8287-2d096b612e9f","metadata":{"id":"b8cc4a6d-dcf5-4a4a-8287-2d096b612e9f"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","from tqdm import tqdm\n","\n","# Check if GPU is available, if not, use CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"id":"585fcdac-5753-485a-86e1-f61d9dcd11fa","metadata":{"colab":{"referenced_widgets":["c9b37c5c96b24e788c0ebd2250950669","c40a3b76bdbc4c98ac617897ff4589ed","e72b07d73d0f46ffbf81b978973f5733","0eb0bfd027354c619137fe563175a818"]},"id":"585fcdac-5753-485a-86e1-f61d9dcd11fa","outputId":"66b6ff9e-d285-4eb5-8729-2dd9b8db7c75"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.18.5 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/notebooks/wandb/run-20241027_002444-1b6y3u0a</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/legalts/transformers_tutorials_summarization/runs/1b6y3u0a\" target=\"_blank\">northern-eon-24</a></strong> to <a href=\"https://wandb.ai/legalts/transformers_tutorials_summarization\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9b37c5c96b24e788c0ebd2250950669","version_major":2,"version_minor":0},"text/plain":["Downloading config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c40a3b76bdbc4c98ac617897ff4589ed","version_major":2,"version_minor":0},"text/plain":["Downloading spiece.model:   0%|          | 0.00/773k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e72b07d73d0f46ffbf81b978973f5733","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer.json:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["                                         defense_pov  \\\n","0  From the defense attorney's perspective, this ...   \n","1  From the defense attorney's perspective, this ...   \n","2  From the perspective of the defense attorney, ...   \n","3  From the defense attorney's perspective, this ...   \n","4  From the defense attorney's perspective, this ...   \n","\n","                                           judgement  \n","0  summarize: vil Appeal Nos. 1227 to 1230 of 197...  \n","1  summarize: Appeal No. 2 of 1968.\\nAppeal under...  \n","2  summarize: Civil Appeal Nos. 33763382 of 1988....  \n","3  summarize: ition No. 2701 of 1981.\\n(Under art...  \n","4  summarize: riminal Appeal No. 3 of 1957.\\nAppe...  \n","FULL Dataset: (3833, 2)\n","TRAIN Dataset: (3641, 2)\n","TEST Dataset: (192, 2)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0eb0bfd027354c619137fe563175a818","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/850M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Initiating Fine-Tuning for the model on our dataset\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2323: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py:667: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss:  306.6994323730469\n","Epoch: 0, Loss:  68.10641479492188\n","Epoch: 0, Loss:  37.87799072265625\n","Epoch: 0, Loss:  32.39731216430664\n","Epoch: 0, Loss:  24.24736785888672\n","Epoch: 0, Loss:  23.255447387695312\n","Epoch: 0, Loss:  21.900392532348633\n","Epoch: 0, Loss:  21.662246704101562\n","Epoch: 0, Loss:  19.41630744934082\n","Epoch: 0, Loss:  16.870193481445312\n","Epoch: 0, Loss:  17.751323699951172\n","Epoch: 0, Loss:  17.544418334960938\n","Epoch: 0, Loss:  17.838253021240234\n","Epoch: 0, Loss:  15.480382919311523\n","Epoch: 0, Loss:  15.651923179626465\n","Epoch: 0, Loss:  14.738114356994629\n","Epoch: 0, Loss:  14.300710678100586\n","Epoch: 0, Loss:  13.004741668701172\n","Epoch: 0, Loss:  13.22833251953125\n","Epoch: 0, Loss:  13.062636375427246\n","Epoch: 0, Loss:  13.843392372131348\n","Epoch: 0, Loss:  12.560341835021973\n","Epoch: 0, Loss:  12.073155403137207\n","Epoch: 0, Loss:  12.169763565063477\n","Epoch: 0, Loss:  11.445817947387695\n","Epoch: 0, Loss:  10.357999801635742\n","Epoch: 0, Loss:  9.651850700378418\n","Epoch: 0, Loss:  9.802482604980469\n","Epoch: 0, Loss:  9.795882225036621\n","Epoch: 0, Loss:  8.63935375213623\n","Epoch: 0, Loss:  9.788040161132812\n","Epoch: 0, Loss:  9.530168533325195\n","Epoch: 0, Loss:  10.186057090759277\n","Epoch: 0, Loss:  10.630255699157715\n","Epoch: 0, Loss:  9.074451446533203\n","Epoch: 0, Loss:  8.870665550231934\n","Epoch: 0, Loss:  7.626538276672363\n","Epoch: 1, Loss:  8.359633445739746\n","Epoch: 1, Loss:  8.603795051574707\n","Epoch: 1, Loss:  8.034887313842773\n","Epoch: 1, Loss:  8.152812957763672\n","Epoch: 1, Loss:  8.597387313842773\n","Epoch: 1, Loss:  7.172802925109863\n","Epoch: 1, Loss:  8.623218536376953\n","Epoch: 1, Loss:  7.083128452301025\n","Epoch: 1, Loss:  7.999099254608154\n","Epoch: 1, Loss:  8.449971199035645\n","Epoch: 1, Loss:  6.343446254730225\n","Epoch: 1, Loss:  6.574280738830566\n","Epoch: 1, Loss:  6.356205940246582\n","Epoch: 1, Loss:  6.633579730987549\n","Epoch: 1, Loss:  6.782663345336914\n","Epoch: 1, Loss:  7.470614433288574\n","Epoch: 1, Loss:  6.1698737144470215\n","Epoch: 1, Loss:  5.80445671081543\n","Epoch: 1, Loss:  7.725068092346191\n","Epoch: 1, Loss:  7.122828006744385\n","Epoch: 1, Loss:  6.3243560791015625\n","Epoch: 1, Loss:  5.4747161865234375\n","Epoch: 1, Loss:  6.983334064483643\n","Epoch: 1, Loss:  5.909689426422119\n","Epoch: 1, Loss:  4.656423568725586\n","Epoch: 1, Loss:  3.8638856410980225\n","Epoch: 1, Loss:  4.847347736358643\n","Epoch: 1, Loss:  5.900091171264648\n","Epoch: 1, Loss:  4.943655490875244\n","Epoch: 1, Loss:  4.7789788246154785\n","Epoch: 1, Loss:  4.094771862030029\n","Epoch: 1, Loss:  4.714572429656982\n","Epoch: 1, Loss:  3.8511812686920166\n","Epoch: 1, Loss:  5.504867076873779\n","Epoch: 1, Loss:  4.576379299163818\n","Epoch: 1, Loss:  4.347414016723633\n","Epoch: 1, Loss:  5.069273948669434\n","Epoch: 2, Loss:  3.573991298675537\n","Epoch: 2, Loss:  4.542819976806641\n","Epoch: 2, Loss:  5.3981404304504395\n","Epoch: 2, Loss:  3.9150021076202393\n","Epoch: 2, Loss:  3.841700792312622\n","Epoch: 2, Loss:  4.34990930557251\n","Epoch: 2, Loss:  4.4119415283203125\n","Epoch: 2, Loss:  4.730745315551758\n","Epoch: 2, Loss:  4.54439115524292\n","Epoch: 2, Loss:  4.375085353851318\n","Epoch: 2, Loss:  3.893428087234497\n","Epoch: 2, Loss:  4.088982105255127\n","Epoch: 2, Loss:  3.2110636234283447\n","Epoch: 2, Loss:  3.778654098510742\n","Epoch: 2, Loss:  3.5888993740081787\n","Epoch: 2, Loss:  3.0989601612091064\n","Epoch: 2, Loss:  4.696619510650635\n","Epoch: 2, Loss:  3.3881449699401855\n","Epoch: 2, Loss:  2.981976270675659\n","Epoch: 2, Loss:  4.101191997528076\n","Epoch: 2, Loss:  3.575270891189575\n","Epoch: 2, Loss:  3.6048038005828857\n","Epoch: 2, Loss:  3.6162381172180176\n","Epoch: 2, Loss:  3.517704963684082\n","Epoch: 2, Loss:  4.771614074707031\n","Epoch: 2, Loss:  3.6899735927581787\n","Epoch: 2, Loss:  4.099143981933594\n","Epoch: 2, Loss:  3.26987624168396\n","Epoch: 2, Loss:  3.34293532371521\n","Epoch: 2, Loss:  3.5396487712860107\n","Epoch: 2, Loss:  2.9949605464935303\n","Epoch: 2, Loss:  3.3471105098724365\n","Epoch: 2, Loss:  3.2992353439331055\n","Epoch: 2, Loss:  3.89082932472229\n","Epoch: 2, Loss:  3.439716339111328\n","Epoch: 2, Loss:  3.834822654724121\n","Epoch: 2, Loss:  3.0238757133483887\n","Epoch: 3, Loss:  4.101586818695068\n","Epoch: 3, Loss:  3.078361749649048\n","Epoch: 3, Loss:  3.756686210632324\n","Epoch: 3, Loss:  3.3081600666046143\n","Epoch: 3, Loss:  2.5934817790985107\n","Epoch: 3, Loss:  3.055196523666382\n","Epoch: 3, Loss:  2.872317314147949\n","Epoch: 3, Loss:  3.1665799617767334\n","Epoch: 3, Loss:  2.6570122241973877\n","Epoch: 3, Loss:  2.694772720336914\n","Epoch: 3, Loss:  2.132683753967285\n","Epoch: 3, Loss:  3.421165704727173\n","Epoch: 3, Loss:  2.6111631393432617\n","Epoch: 3, Loss:  2.991137981414795\n","Epoch: 3, Loss:  2.8396012783050537\n","Epoch: 3, Loss:  2.920060396194458\n","Epoch: 3, Loss:  3.2640509605407715\n","Epoch: 3, Loss:  2.2823421955108643\n","Epoch: 3, Loss:  2.6484649181365967\n","Epoch: 3, Loss:  2.352869987487793\n","Epoch: 3, Loss:  2.6037135124206543\n","Epoch: 3, Loss:  2.770534038543701\n","Epoch: 3, Loss:  3.1755902767181396\n","Epoch: 3, Loss:  3.4000699520111084\n","Epoch: 3, Loss:  2.632972240447998\n","Epoch: 3, Loss:  2.5529847145080566\n","Epoch: 3, Loss:  3.023668050765991\n","Epoch: 3, Loss:  3.372899055480957\n","Epoch: 3, Loss:  2.6197803020477295\n","Epoch: 3, Loss:  2.052172899246216\n","Epoch: 3, Loss:  2.2380776405334473\n","Epoch: 3, Loss:  2.84999680519104\n","Epoch: 3, Loss:  3.1229701042175293\n","Epoch: 3, Loss:  2.293699026107788\n","Epoch: 3, Loss:  2.5235230922698975\n","Epoch: 3, Loss:  2.5254292488098145\n","Epoch: 3, Loss:  1.8324501514434814\n","Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n","Completed 0\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2323: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py:667: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Completed 100\n","Output Files generated for review\n","                                        Generated Text  \\\n","0    The defense could argue that the income tax li...   \n","1    The defense could argue that the absence of a ...   \n","2    The defense could argue that the judgment fail...   \n","3    The defense could argue that the judge's inter...   \n","4    Movers' case, which could be used as a basis f...   \n","..                                                 ...   \n","187  The defense could argue that the Income Tax Ac...   \n","188  The defense could argue that the subletting of...   \n","189  The defense could argue that the judgment fail...   \n","190  The defense could argue that the minimum wages...   \n","191  The defense could argue that the judgment fail...   \n","\n","                                           Actual Text  \n","0    From the perspective of the defense attorney, ...  \n","1    From the perspective of the defense attorney, ...  \n","2    From the perspective of the defense attorney, ...  \n","3    From the perspective of the defense attorney, ...  \n","4    From the defense attorney's perspective, this ...  \n","..                                                 ...  \n","187  From the perspective of the defense attorney, ...  \n","188  From a defense attorney's standpoint, this jud...  \n","189  From the defense attorney's perspective, this ...  \n","190  From the defense attorney's perspective, this ...  \n","191  From the perspective of the defense attorney, ...  \n","\n","[192 rows x 2 columns]\n","Model and tokenizer saved at: T5_Large_local_def\n"]}],"source":["def main():\n","    wandb.init(project=\"transformers_tutorials_summarization\")\n","    config = wandb.config          # Initialize config\n","    config.TRAIN_BATCH_SIZE = 1    # input batch size for training (default: 64)\n","    config.VALID_BATCH_SIZE = 1    # input batch size for testing (default: 1000)\n","    config.TRAIN_EPOCHS = 4       # number of epochs to train (default: 10)\n","    config.VAL_EPOCHS = 1\n","    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","    config.SEED = 42               # random seed (default: 42)\n","    config.MAX_LEN = 10000\n","    config.SUMMARY_LEN = 1000\n","\n","    # Set random seeds and deterministic pytorch for reproducibility\n","    torch.manual_seed(config.SEED) # pytorch random seed\n","    np.random.seed(config.SEED) # numpy random seed\n","    torch.backends.cudnn.deterministic = True\n","\n","    # tokenzier for encoding the text\n","    tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n","\n","\n","\n","    df = pd.read_csv(\"train_data.csv\")\n","    df = df[['Perspective-based Summary','Judgement']]\n","    df.Judgement = 'summarize: ' + df.judgement\n","    print(df.head())\n","    train_size = 0.95\n","    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n","    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","    train_dataset = train_dataset.reset_index(drop=True)\n","\n","    print(\"FULL Dataset: {}\".format(df.shape))\n","    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n","\n","    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","    train_params = {\n","        'batch_size': config.TRAIN_BATCH_SIZE,\n","        'shuffle': True,\n","        'num_workers': 0\n","        }\n","\n","    val_params = {\n","        'batch_size': config.VALID_BATCH_SIZE,\n","        'shuffle': False,\n","        'num_workers': 0\n","        }\n","    training_loader = DataLoader(training_set, **train_params)\n","    val_loader = DataLoader(val_set, **val_params)\n","\n","    torch.cuda.empty_cache()\n","\n","    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/long-t5-local-base\")\n","    model = model.to(device)\n","\n","\n","    torch.cuda.empty_cache()\n","    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n","\n","    wandb.watch(model, log=\"all\")\n","\n","    print('Initiating Fine-Tuning for the model on our dataset')\n","\n","    for epoch in range(config.TRAIN_EPOCHS):\n","        train(epoch, tokenizer, model, device, training_loader, optimizer)\n","\n","    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n","    for epoch in range(config.VAL_EPOCHS):\n","        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n","        print('Output Files generated for review')\n","\n","    print(final_df)\n","    # Save the trained model\n","    model_path = \"T5_Large_local_def\"\n","    model.save_pretrained(model_path)\n","    tokenizer.save_pretrained(model_path)\n","    print(f\"Model and tokenizer saved at: {model_path}\")\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"id":"638be18d-9c6f-476b-9336-c39f0e399e00","metadata":{"id":"638be18d-9c6f-476b-9336-c39f0e399e00","outputId":"c7e40567-8c0d-4710-85db-6b49aea81b3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","  adding: T5_Large_local_def/ (stored 0%)\n","  adding: T5_Large_local_def/pytorch_model.bin (deflated 7%)\n","  adding: T5_Large_local_def/tokenizer.json (deflated 74%)\n","  adding: T5_Large_local_def/config.json (deflated 49%)\n","  adding: T5_Large_local_def/tokenizer_config.json (deflated 83%)\n","  adding: T5_Large_local_def/special_tokens_map.json (deflated 86%)\n","  adding: T5_Large_local_def/spiece.model (deflated 48%)\n"]}],"source":["!zip -r T5_Large_local_def.zip T5_Large_local_def"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}