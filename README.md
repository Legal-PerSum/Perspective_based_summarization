# Perspective_based_summarization

## Abstract
Perspective-Aware Summarization (PaS) of legal documents has emerged as a critical task in Legal Natural Language Processing (Legal-NLP), enabling stakeholders such as legal practitioners and lay readers to interpret complex judicial decisions through role-specific lenses. However, the field lacks structured, large-scale datasets that capture these varying perspectives across diverse case types. In this work, we introduce Legal-PerSum, a semi-synthetic dataset of Indian Supreme Court judgments curated using Large Language Models (LLMs) to generate summaries conditioned on stakeholder roles such as prosecution, defense, plaintiff, petitioner, or respondent depending on the case context and categorization. We benchmark multiple state-of-the-art LLMs on this dataset under both prompting (zero-shot) and fine-tuning setups. Our evaluation combines human judgment with entailment-based automated metrics to assess alignment, hallucination, and summary quality. Results show that fine-tuned models substantially outperform zero-shot baselines in producing coherent and stakeholder-aligned summaries. This work sets a new benchmark for role-specific legal summarization, highlighting key limitations of current LLMs in modeling nuanced legal perspectives, and lays a foundation for future advancements in interpretable and controllable Legal-NLP.
