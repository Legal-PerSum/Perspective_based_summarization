{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab57300-daed-4b91-8b20-49e69c116fca",
   "metadata": {
    "id": "0ab57300-daed-4b91-8b20-49e69c116fca"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import io\n",
    "\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd24365-dbd2-4103-a727-907a8b72b7c9",
   "metadata": {
    "id": "5cd24365-dbd2-4103-a727-907a8b72b7c9"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.defense_pov = self.data.defense_pov\n",
    "        self.judgement = self.data.judgement\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.defense_pov)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        judgement = str(self.judgement[index])\n",
    "        judgement = ' '.join(judgement.split())\n",
    "\n",
    "        defense_pov = str(self.defense_pov[index])\n",
    "        defense_pov = ' '.join(defense_pov.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([judgement], max_length= self.source_len,truncation=True, pad_to_max_length=True,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([defense_pov], max_length= self.summ_len,truncation=True, pad_to_max_length=True,return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long),\n",
    "            'source_mask': source_mask.to(dtype=torch.long),\n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f17ff6-3a08-4923-9488-ff1f6c5b5bdb",
   "metadata": {
    "id": "04f17ff6-3a08-4923-9488-ff1f6c5b5bdb"
   },
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        labels = y[:, 1:].clone().detach()\n",
    "        labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if _%10 == 0:\n",
    "            wandb.log({\"Training Loss\": loss.item()})\n",
    "\n",
    "        if _%100==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8400898-7d7e-4f47-a133-e4b13c204dd5",
   "metadata": {
    "id": "c8400898-7d7e-4f47-a133-e4b13c204dd5"
   },
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask,\n",
    "                max_length=150,\n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            if _%100==0:\n",
    "                print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc4a6d-dcf5-4a4a-8287-2d096b612e9f",
   "metadata": {
    "id": "b8cc4a6d-dcf5-4a4a-8287-2d096b612e9f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available, if not, use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585fcdac-5753-485a-86e1-f61d9dcd11fa",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "c9b37c5c96b24e788c0ebd2250950669",
      "c40a3b76bdbc4c98ac617897ff4589ed",
      "e72b07d73d0f46ffbf81b978973f5733",
      "0eb0bfd027354c619137fe563175a818"
     ]
    },
    "id": "585fcdac-5753-485a-86e1-f61d9dcd11fa",
    "outputId": "66b6ff9e-d285-4eb5-8729-2dd9b8db7c75"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    wandb.init(project=\"transformers_tutorials_summarization\")\n",
    "    config = wandb.config          # Initialize config\n",
    "    config.TRAIN_BATCH_SIZE = 1    # input batch size for training (default: 64)\n",
    "    config.VALID_BATCH_SIZE = 1    # input batch size for testing (default: 1000)\n",
    "    config.TRAIN_EPOCHS = 4       # number of epochs to train (default: 10)\n",
    "    config.VAL_EPOCHS = 1\n",
    "    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "    config.SEED = 42               # random seed (default: 42)\n",
    "    config.MAX_LEN = 10000\n",
    "    config.SUMMARY_LEN = 1000\n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(config.SEED) # pytorch random seed\n",
    "    np.random.seed(config.SEED) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.read_csv(\"train_data.csv\")\n",
    "    df = df[['Perspective-based Summary','Judgement']]\n",
    "    df.Judgement = 'summarize: ' + df.judgement\n",
    "    print(df.head())\n",
    "    train_size = 0.95\n",
    "    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n",
    "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    print(\"FULL Dataset: {}\".format(df.shape))\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
    "\n",
    "    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "    train_params = {\n",
    "        'batch_size': config.TRAIN_BATCH_SIZE,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    val_params = {\n",
    "        'batch_size': config.VALID_BATCH_SIZE,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/long-t5-local-base\")\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "    wandb.watch(model, log=\"all\")\n",
    "\n",
    "    print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "    for epoch in range(config.TRAIN_EPOCHS):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "    for epoch in range(config.VAL_EPOCHS):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        print('Output Files generated for review')\n",
    "\n",
    "    print(final_df)\n",
    "    # Save the trained model\n",
    "    model_path = \"T5_Large_local_def\"\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    print(f\"Model and tokenizer saved at: {model_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638be18d-9c6f-476b-9336-c39f0e399e00",
   "metadata": {
    "id": "638be18d-9c6f-476b-9336-c39f0e399e00",
    "outputId": "c7e40567-8c0d-4710-85db-6b49aea81b3d"
   },
   "outputs": [],
   "source": [
    "!zip -r T5_Large_local_def.zip T5_Large_local_def"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
